{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onfy1vqLnYJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63212b92-2d44-4251-b206-0fc2f7fdd0fa"
      },
      "source": [
        "#import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "outputId": "7110f103-c47d-4aea-9c09-36812a9dc148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# This shows there are 60,000 samples in the training data set.\n",
        "x_train.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "outputId": "20e263e1-c6d6-4182-d1e7-4c6ba8106bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# This shows there are 10,000 samples in the training data set.\n",
        "\n",
        "x_test.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "outputId": "48bd86bf-d324-41b6-d6b5-6d071c8d8811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "firstObj = x_test[0]\n",
        "\n",
        "print(firstObj.shape)\n",
        "\n",
        "# Dimension of an image is 28 vs 28."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert labels to one hot encoding\n",
        "import keras\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test =keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fUQpMHxJjwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changing the datatype of x_train and x_test first to float 32.\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okwo_SB5JjwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize\n",
        "\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will be done while building the model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vYOiNdPt16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the Model \n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3x3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Clear out tensorflow memory\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Define the Type of Model\n",
        "model1 = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWJP6dH3vnLu",
        "colab_type": "code",
        "outputId": "87641fe2-52e2-4f1d-d67e-8ff692a75ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# 1st Conv Layer\n",
        "    model1.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model1.add(Activation('relu'))\n",
        "\n",
        "    # 2nd Conv Layer\n",
        "    model1.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model1.add(Activation('relu'))\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model1.add(Flatten())\n",
        "    model1.add(Dense(128))\n",
        "    model1.add(Activation('relu'))\n",
        "\n",
        "    # Prediction Layer\n",
        "    model1.add(Dense(10))\n",
        "    model1.add(Activation('softmax'))\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "     # Store Training Results\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyhGEzL3x1Rv",
        "colab_type": "code",
        "outputId": "002a895a-4464-4911-ddb6-386e91a5e393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "# Train the model\n",
        "model1.fit(x_train, y_train, batch_size=32, nb_epoch=10, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 28s 462us/step - loss: 0.3698 - acc: 0.8662 - val_loss: 0.2785 - val_acc: 0.8977\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.2289 - acc: 0.9155 - val_loss: 0.2661 - val_acc: 0.9062\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.1645 - acc: 0.9400 - val_loss: 0.2465 - val_acc: 0.9146\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.1156 - acc: 0.9566 - val_loss: 0.2703 - val_acc: 0.9101\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 0.0754 - acc: 0.9723 - val_loss: 0.3017 - val_acc: 0.9186\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 20s 332us/step - loss: 0.0513 - acc: 0.9813 - val_loss: 0.3641 - val_acc: 0.9107\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0339 - acc: 0.9873 - val_loss: 0.4202 - val_acc: 0.9193\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 20s 331us/step - loss: 0.0272 - acc: 0.9909 - val_loss: 0.4778 - val_acc: 0.9057\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.0214 - acc: 0.9926 - val_loss: 0.4728 - val_acc: 0.9090\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.0197 - acc: 0.9931 - val_loss: 0.4965 - val_acc: 0.9093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa760e48240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the Type of Model\n",
        "model2 = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq1Esq3386Pn",
        "colab_type": "code",
        "outputId": "3bc25890-ab07-44d4-b121-0c1778054c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "source": [
        "\n",
        "# Define the Type of Model\n",
        "model2 = Sequential()\n",
        "\n",
        "# 1st Conv Layer\n",
        "model2.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "model2.add(Activation('relu')) \n",
        "\n",
        "# 2nd Conv Layer\n",
        "model2.add(Convolution2D(32, 3, 3))\n",
        "model2.add(Activation('relu')) \n",
        "\n",
        "# Max Pooling\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Dropout\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "# Fully Connected Layer\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128))\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "# Prediction Layer\n",
        "model2.add(Dense(10))\n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "# Loss and Optimizer\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Store Training Results\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]\n",
        "\n",
        "# Train the model\n",
        "model2.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.3926 - acc: 0.8590 - val_loss: 0.3101 - val_acc: 0.8868\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.2587 - acc: 0.9049 - val_loss: 0.2560 - val_acc: 0.9041\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2120 - acc: 0.9207 - val_loss: 0.2574 - val_acc: 0.9069\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 19s 316us/step - loss: 0.1786 - acc: 0.9335 - val_loss: 0.2386 - val_acc: 0.9175\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 19s 315us/step - loss: 0.1534 - acc: 0.9427 - val_loss: 0.2275 - val_acc: 0.9194\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 19s 315us/step - loss: 0.1312 - acc: 0.9496 - val_loss: 0.2258 - val_acc: 0.9225\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 19s 312us/step - loss: 0.1119 - acc: 0.9580 - val_loss: 0.2306 - val_acc: 0.9231\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 19s 315us/step - loss: 0.0965 - acc: 0.9641 - val_loss: 0.2579 - val_acc: 0.9201\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0825 - acc: 0.9686 - val_loss: 0.2552 - val_acc: 0.9261\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 19s 315us/step - loss: 0.0728 - acc: 0.9725 - val_loss: 0.2687 - val_acc: 0.9248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa751a42550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXPpfkNtDEL3",
        "colab_type": "code",
        "outputId": "12ca18fb-979b-4d79-d14a-12ae47a47b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000, 10)\n",
            "(10000, 28, 28, 1)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbz4uHBuJjwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.01,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.01,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "bcabfc54-88d0-4779-e3b3-542186179764"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAACeCAYAAADXJlBrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Complile the model\n",
        "model3 = Sequential()\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ktKQNieUQiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "1c4d4ca3-c747-4da3-c6fd-15c9ca034f5c"
      },
      "source": [
        "model3.fit_generator(datagen.flow(x_train, y_train,batch_size=32),\n",
        "                    samples_per_epoch=x_train.shape[0],\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=10, validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=1562)`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-6a07bb6bec1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_data=(x_test, y_test), callbacks=callback_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must compile your model before using it.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before using it."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1SrtBEPJjwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_and_metrics = model3.evaluate(x_train, y_train)\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwVWNQC2qZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "60472f99-77ae-408d-d8dd-a3970978869b"
      },
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Pht1ggHuiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n28ccU6Hp6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen1 = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.01,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.01,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=True)  # randomly flip images\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the generator\n",
        "datagen1.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "34ed66d3-2763-471e-8dec-2f6f6bde90fb"
      },
      "source": [
        "gen = datagen1.flow(x_train[:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAACeCAYAAADXJlBrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADpUlEQVR4nO3cQQrCMBBA0VZ6Ng/t5eJeBPFXGoLv\n7UoXmcVQ+AS6jzE2AAAAvnObPQAAAMCKxBQAAEAgpgAAAAIxBQAAEIgpAACA4Pjw3q/+OGu/+Dw7\ny1lX7+y22VvO861lNXaW1bzdWTdTAAAAgZgCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAE\nYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBMAQAABGIKAAAgEFMAAACBmAIA\nAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAgZgCAAAIxBQAAEAgpgAAAAIx\nBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBMAQAA\nBGIKAAAgEFMAAACBmAIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAgZgC\nAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAAC\nMQUAABCIKQAAgEBMAQAABGIKAAAgEFMAAACBmAIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEA\nAARiCgAAIBBTAAAAgZgCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGY\nAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBMAQAABGIKAAAgEFMAAACBmAIAAAjEFAAAQCCmAAAA\nAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAgZgCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwB\nAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBMAQAABGIKAAAgEFMAAACB\nmAIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAgZgCAAAIxBQAAEAgpgAA\nAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBM\nAQAABGIKAAAgEFMAAACBmAIAAAjEFAAAQHDMHgDgN+4vz48pUwAA/8PNFAAAQCCmAAAAAjEFAAAQ\niCkAAIBgH2PMngEAAGA5bqYAAAACMQUAABCIKQAAgEBMAQAABGIKAAAgEFMAAADBE8qlDTkAZps/\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dswo8n3Z7x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}